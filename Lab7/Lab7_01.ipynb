{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab7_01.ipynb","provenance":[],"authorship_tag":"ABX9TyOYwkvWGBHJsaDCxyFIimvB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDW4ExbUajIK","executionInfo":{"status":"ok","timestamp":1630601393479,"user_tz":-330,"elapsed":4964,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}},"outputId":"30e5c318-11d4-4bf3-a4e6-229ce581192b"},"source":["import nltk\n","from nltk.corpus import twitter_samples \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVxBdiRPaqRQ","executionInfo":{"status":"ok","timestamp":1630601398021,"user_tz":-330,"elapsed":1073,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}},"outputId":"df096371-7868-48f7-c08c-5821be280290"},"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"xpvMLBpaauIu","executionInfo":{"status":"ok","timestamp":1630601410682,"user_tz":-330,"elapsed":1448,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"DM2I4chKaye-","executionInfo":{"status":"ok","timestamp":1630601428930,"user_tz":-330,"elapsed":1118,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","\n","\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if(word not in stopwords_english and word not in string.punctuation):\n","          stem_word = stemmer.stem(word)\n","          tweets_clean.append(stem_word)\n","            #############################################################\n","            # 1 remove stopwords\n","            # 2 remove punctuation\n","            # 3 stemming word\n","            # 4 Add it to tweets_clean\n","\n","    return tweets_clean"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"7H2Ilw3Oa5at","executionInfo":{"status":"ok","timestamp":1630601458995,"user_tz":-330,"elapsed":1141,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","  # a positive label '1'         or \n","  # a negative label '0', \n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple, \n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","              freqs[pair] += 1\n","            else:\n","              freqs[pair] = 1\n","            \n","            #############################################################\n","            #Update the count of pair if present, set it to 1 otherwise\n","            \n","\n","    return freqs"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zyjzss5va9ix","executionInfo":{"status":"ok","timestamp":1630601472931,"user_tz":-330,"elapsed":854,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXGwh0bcbDRb"},"source":["\n","\n","*   Train test split: 20% will be in the test set, and 80% in the training set.\n","\n"]},{"cell_type":"code","metadata":{"id":"-BFzMnbWbJBe","executionInfo":{"status":"ok","timestamp":1630601530122,"user_tz":-330,"elapsed":792,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["# split the data into two pieces, one for training and one for testing\n","#############################################################\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTqz6TVsbQ53"},"source":["\n","\n","*   Create the numpy array of positive labels and negative labels.\n","\n"]},{"cell_type":"code","metadata":{"id":"l-hnNczKbVFX","executionInfo":{"status":"ok","timestamp":1630601583170,"user_tz":-330,"elapsed":961,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n","Final_data = all_positive_tweets+all_negative_tweets\n","data =np.append(np.ones((len(all_positive_tweets), 1)), np.zeros((len(all_negative_tweets), 1)), axis=0)\n","train_x,test_x,train_y,test_y = train_test_split(Final_data,data,test_size=0.25,random_state= 76)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k66KJ6DPbbb_"},"source":["\n","\n","*   Create the frequency dictionary using the build_freqs() function.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QtPggOg9beQQ","executionInfo":{"status":"ok","timestamp":1630601624666,"user_tz":-330,"elapsed":4178,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}},"outputId":"df1d0390-f80b-4433-807d-a3dc03752988"},"source":["# create frequency dictionary\n","#############################################################\n","freqs = build_freqs(train_x , train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))\n","def extract_features(tweet, freqs): \n","    word_l = process_tweet(tweet)\n","    x = np.zeros((1, 2)) \n","    for word in word_l:\n","        if((word,1) in freqs):\n","          x[0,0]+=freqs[word,1]\n","        if((word,0) in freqs):\n","          x[0,1]+=freqs[word,0]\n","    \n","    assert(x.shape == (1, 2))\n","    return x[0]\n","\n","#pred function\n","def predict_tweet(tweet):\n","    with tf.Session() as sess:\n","      saver.restore(sess,save_path='TSession')\n","      data_i=[]\n","      for t in tweet:\n","        data_i.append(extract_features(t,freqs))\n","      data_i=np.asarray(data_i)\n","      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),bias)))\n","    print(\"--Fail--\")\n","    return"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 10919\n"]}]},{"cell_type":"code","metadata":{"id":"IJXzQGyAbk_c","executionInfo":{"status":"ok","timestamp":1630601636475,"user_tz":-330,"elapsed":863,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["bias=tf.Variable(np.random.randn(1),name=\"Bias\")\n","W=tf.Variable(np.random.randn(1,2),name=\"Weight\")\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"epvK2Ts5bohe","executionInfo":{"status":"ok","timestamp":1630601669775,"user_tz":-330,"elapsed":4594,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}}},"source":["data=[]\n","for t in train_x:\n","  data.append(extract_features(t,freqs))\n","data=np.asarray(data)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eh9n5bJobuLE","executionInfo":{"status":"ok","timestamp":1630601679421,"user_tz":-330,"elapsed":838,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}},"outputId":"cc5009ba-9ad2-4f02-9de7-a1cdd5e1abc2"},"source":["Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), bias)) \n","ta=np.asarray(train_y)\n","Total_cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = ta) \n","print(Total_cost)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"logistic_loss:0\", shape=(7500, 1), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"id":"grnsVtt7bzss","executionInfo":{"status":"ok","timestamp":1630601712589,"user_tz":-330,"elapsed":10786,"user":{"displayName":"CE076_Krupali_Mehta","photoUrl":"","userId":"13507360727960861074"}},"outputId":"d3480e09-e361-4d80-a332-bb40b4598888","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Gradient Descent Optimizer \n","optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.00001 ,name=\"GradientDescent\").minimize(Total_cost) \n","# Global Variables Initializer \n","init = tf.global_variables_initializer()\n","\n","saver = tf.train.Saver()\n","with tf.Session() as sess:\n","  \n","  sess.run(init)\n","  print(\"Bias\",sess.run(bias))\n","  print(\"Weight\",sess.run(W))\n","  for epoch in range(1000):\n","    sess.run(optimizer)\n","    preds=sess.run(Y_hat)\n","    acc=((preds==ta).sum())/len(train_y)\n","    Accuracy=[]\n","    repoch=False\n","    if repoch:\n","      Accuracy.append(acc)\n","    if epoch % 1000 == 0:\n","      print(\"Accuracy\",acc)\n","    saved_path = saver.save(sess, 'TSession')\n","\n","preds=predict_tweet(test_x)\n","print(preds,len(test_y))\n"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Bias [-0.6933117]\n","Weight [[0.17635655 1.36535196]]\n","Accuracy 0.4928\n","INFO:tensorflow:Restoring parameters from TSession\n","[[1.]\n"," [1.]\n"," [1.]\n"," ...\n"," [1.]\n"," [1.]\n"," [1.]] 2500\n"]}]}]}